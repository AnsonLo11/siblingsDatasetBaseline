{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要分别读入face和pose的数据并对应起来\n",
    "- 对于siblings_face 需要读入json  \n",
    "- 对于siblings_pose 需要读入csv    \n",
    "- 对于Label 需要读入csv\n",
    "\n",
    "\n",
    "*特别注意对于face，需要去除置信度小于阈值的数据  \n",
    "*要将video的label扩展到每个frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 读取标签文件\n",
    "label = pd.read_csv('labelA/回应情况-表格 1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>切片ID</th>\n",
       "      <th>回应情况</th>\n",
       "      <th>Unnamed: 2</th>\n",
       "      <th>Unnamed: 3</th>\n",
       "      <th>Unnamed: 4</th>\n",
       "      <th>Unnamed: 5</th>\n",
       "      <th>Unnamed: 6</th>\n",
       "      <th>Unnamed: 7</th>\n",
       "      <th>Unnamed: 8</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>Unnamed: 12</th>\n",
       "      <th>Unnamed: 13</th>\n",
       "      <th>Unnamed: 14</th>\n",
       "      <th>Unnamed: 15</th>\n",
       "      <th>Unnamed: 16</th>\n",
       "      <th>Unnamed: 17</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>NaN</td>\n",
       "      <td>强</td>\n",
       "      <td>中</td>\n",
       "      <td>弱</td>\n",
       "      <td>没有回应（忽视）</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15YS_20230317_01/VCAM_0000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>15YS_20230317_01/VCAM_0000_1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>15YS_20230317_01/VCAM_0000_2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>15YS_20230317_01/VCAM_0000_3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           切片ID 回应情况 Unnamed: 2 Unnamed: 3 Unnamed: 4   \n",
       "0                           NaN    强          中          弱   没有回应（忽视）  \\\n",
       "1    15YS_20230317_01/VCAM_0000    0          0          1          0   \n",
       "2  15YS_20230317_01/VCAM_0000_1    0          0          1          0   \n",
       "3  15YS_20230317_01/VCAM_0000_2    1          0          0          0   \n",
       "4  15YS_20230317_01/VCAM_0000_3    0          0          1          0   \n",
       "\n",
       "   Unnamed: 5  Unnamed: 6  Unnamed: 7  Unnamed: 8  Unnamed: 9  Unnamed: 10   \n",
       "0         NaN         NaN         NaN         NaN         NaN          NaN  \\\n",
       "1         NaN         NaN         NaN         NaN         NaN          NaN   \n",
       "2         NaN         NaN         NaN         NaN         NaN          NaN   \n",
       "3         NaN         NaN         NaN         NaN         NaN          NaN   \n",
       "4         NaN         NaN         NaN         NaN         NaN          NaN   \n",
       "\n",
       "   Unnamed: 11  Unnamed: 12  Unnamed: 13  Unnamed: 14  Unnamed: 15   \n",
       "0          NaN          NaN          NaN          NaN          NaN  \\\n",
       "1          NaN          NaN          NaN          NaN          NaN   \n",
       "2          NaN          NaN          NaN          NaN          NaN   \n",
       "3          NaN          NaN          NaN          NaN          NaN   \n",
       "4          NaN          NaN          NaN          NaN          NaN   \n",
       "\n",
       "   Unnamed: 16  Unnamed: 17  \n",
       "0          NaN          NaN  \n",
       "1          NaN          NaN  \n",
       "2          NaN          NaN  \n",
       "3          NaN          NaN  \n",
       "4          NaN          NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label.iloc[1,0][-4:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入face特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error: siblings_face/15YS_20230317_01/VCAM_0000_3.csv not exist\n",
      "error: siblings_face/15YS_20230317_01/VCAM_0000_37.csv not exist\n",
      "error: siblings_face/15YS_20230317_01/VCAM_0001_29.csv not exist\n",
      "error: siblings_face/15YS_20230317_01/VCAM_0001_30.csv not exist\n",
      "error: siblings_face/15YS_20230317_01/VCAM_0001_37.csv not exist\n",
      "error: siblings_face/15YS_20230317_01/VCAM_0002_53.csv not exist\n",
      "error: siblings_face/15YS_20230317_01/VCAM_0002_76.csv not exist\n",
      "error: siblings_face/15YS_20230317_01/VCAM_0003_89.csv not exist\n",
      "error: siblings_face/16YS_20230317_01/VCAM_0004_126.csv not exist\n",
      "error: siblings_face/16YS_20230317_01/VCAM_0006_18.csv not exist\n",
      "error: siblings_face/16YS_20230317_01/VCAM_0006_81.csv not exist\n",
      "error: siblings_face/16YS_20230317_01/VCAM_0006_103.csv not exist\n",
      "Import 857 files, missing 12 files\n",
      "X:857, y:19325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11007/350757143.py:39: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  X = np.array(features)\n"
     ]
    }
   ],
   "source": [
    "# set confidence threshold\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "# 导入face特征并对齐标签\n",
    "features = []\n",
    "labels = []\n",
    "Mcount = 0\n",
    "Dcount = 0\n",
    "for i in range(1,len(label)):\n",
    "    directory = label.iloc[i, 0]\n",
    "\n",
    "    label_value = label.iloc[i, 1:5] # 取1-4位的标签\n",
    "    feature_file = os.path.join('siblings_face', directory + '.csv')\n",
    "    try:\n",
    "        feature_data = pd.read_csv(feature_file)\n",
    "\n",
    "        # 丢弃置信度小于0.8的数据\n",
    "        feature_data = feature_data[feature_data['confidence'] >= confidence_threshold]\n",
    "        \n",
    "        # 每一frame加入\n",
    "        features.append(feature_data.values) \n",
    "\n",
    "\n",
    "        # 重复frame标签\n",
    "        for j in range(feature_data.shape[0]):\n",
    "            labels.append(label_value.astype(\"int\"))\n",
    "        \n",
    "        Dcount += 1\n",
    "    except:\n",
    "        print(f\"error: {feature_file} not exist\")\n",
    "        Mcount +=1\n",
    "        continue\n",
    "    \n",
    "\n",
    "print(f\"Import {Dcount} files, missing {Mcount} files\")\n",
    "\n",
    "# 将特征和标签转换为numpy数组\n",
    "# X = np.concatenate(features, axis=0)\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"X:{len(X)}, y:{len(y)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "0.8 # 16000\n",
    "0.7 # 17393\n",
    "0.6 # 18295\n",
    "0.5 # 19325"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 导入pose特征"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# 导入特征并对齐标签\n",
    "features = []\n",
    "labels = []\n",
    "Mcount = 0\n",
    "Dcount = 0\n",
    "\n",
    "# for a single video\n",
    "for i in range(1,len(label)):\n",
    "    directory = label.iloc[i, 0] \n",
    "    # VCAM number\n",
    "    Date = directory.split('/')[0] # 15YS_20230317_01\n",
    "    Vcam = directory.split('/')[-1]  # VCAM_xxxx_xx\n",
    "    VcamID = Vcam.split('_')[1] # xxxx\n",
    "\n",
    "    # reconstruct the correct directory\n",
    "    video_directory = os.path.join('siblings_pose', Date,'VCAM_'+VcamID, Vcam)\n",
    "    json_directory = os.path.join(video_directory, 'json')\n",
    "\n",
    "    # label\n",
    "    label_value = label.iloc[i, 1:5] # 取1-4位的标签\n",
    "\n",
    "    json_files = os.listdir(json_directory)\n",
    "    json_files.sort(key=lambda x: int(x.split('_')[2])) # sort by the frame id\n",
    "    pose_json_data = []\n",
    "\n",
    "    # for a single frame\n",
    "    for file in json_files: # for a single frame\n",
    "        file_path = os.path.join(json_directory, file)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # 漏检测 只有1人的情况\n",
    "            if len(data['people']) != 2:\n",
    "                row0 = data['people'][0]['pose_keypoints_2d']\n",
    "                # 插入face_id\n",
    "                row0.insert(0,0)\n",
    "                 # 插入frame\n",
    "                frame_id = int(file.split('_')[-2])//5 +1 # 恢复frame id\n",
    "                row0.insert(0,frame_id)\n",
    "                pose_json_data.append(row0)\n",
    "\n",
    "                continue\n",
    "            \n",
    "            row0 = data['people'][0]['pose_keypoints_2d']\n",
    "            row1 = data['people'][1]['pose_keypoints_2d']\n",
    "\n",
    "\n",
    "            # face_id对应 x较小 标记为1，x较大 标记为0 \n",
    "            if(row0[0]<row1[0]):\n",
    "                row0,row1 = row1,row0 # swap\n",
    "\n",
    "            # 插入face_id\n",
    "            row0.insert(0,0)\n",
    "            row1.insert(0,1)\n",
    "\n",
    "            # 插入frame\n",
    "            frame_id = int(file.split('_')[-2])//5 # 恢复frame id\n",
    "            row0.insert(0,frame_id)\n",
    "            row1.insert(0,frame_id)\n",
    "\n",
    "\n",
    "            # 重复frame标签\n",
    "            # labels.append(label_value.astype(\"int\"))\n",
    "            pose_json_data.append(row0)\n",
    "            pose_json_data.append(row1)\n",
    "\n",
    "        except:\n",
    "            print(f\"pose error: {file} not exist\")\n",
    "            continue\n",
    "\n",
    "    features.append(np.array(pose_json_data))\n",
    "    \n",
    "\n",
    "# 将特征和标签转换为numpy数组\n",
    "X = np.concatenate(features, axis=0)\n",
    "y = np.array(labels)\n",
    "\n",
    "print(f\"Import {Dcount} files, missing {Mcount} files\")\n",
    "print(f\"X:{len(X)}, y:{len(y)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "face error: siblings_face/15YS_20230317_01/VCAM_0000_3.csv not exist\n",
      "face error: siblings_face/15YS_20230317_01/VCAM_0000_37.csv not exist\n",
      "face error: siblings_face/15YS_20230317_01/VCAM_0001_29.csv not exist\n",
      "face error: siblings_face/15YS_20230317_01/VCAM_0001_30.csv not exist\n",
      "face error: siblings_face/15YS_20230317_01/VCAM_0001_37.csv not exist\n",
      "face error: siblings_face/15YS_20230317_01/VCAM_0002_53.csv not exist\n",
      "face error: siblings_face/15YS_20230317_01/VCAM_0002_76.csv not exist\n",
      "face error: siblings_face/15YS_20230317_01/VCAM_0003_89.csv not exist\n",
      "face error: siblings_face/16YS_20230317_01/VCAM_0004_126.csv not exist\n",
      "pose error: VCAM_0005_164_000000000166_keypoints.json not exist\n",
      "face error: siblings_face/16YS_20230317_01/VCAM_0006_18.csv not exist\n",
      "face error: siblings_face/16YS_20230317_01/VCAM_0006_81.csv not exist\n",
      "face error: siblings_face/16YS_20230317_01/VCAM_0006_103.csv not exist\n",
      "pose error: VCAM_0006_142_000000000190_keypoints.json not exist\n",
      "Total face feat 19255, pose feat 51216\n",
      "Read in X:17673, y:17673\n",
      "Drop 1582 face features, 33543 pose features\n"
     ]
    }
   ],
   "source": [
    "# set openface confidence threshold\n",
    "confidence_threshold = 0.5\n",
    "\n",
    "# 导入特征并对齐标签\n",
    "features = []\n",
    "labels = []\n",
    "frame2video_id = [] # 记录frame属于哪个video\n",
    "# 记录原本有多少条记录\n",
    "face_count = 0\n",
    "pose_count = 0\n",
    "\n",
    "\n",
    "for i in range(1,len(label)):\n",
    "    directory = label.iloc[i, 0]\n",
    "    # VCAM number\n",
    "    Date = directory.split('/')[0] # 15YS_20230317_01\n",
    "    Vcam = directory.split('/')[-1]  # VCAM_xxxx_xx\n",
    "    VcamID = Vcam.split('_')[1] # xxxx\n",
    "\n",
    "    # reconstruct the correct Openpose directory\n",
    "    video_directory = os.path.join('siblings_pose', Date,'VCAM_'+VcamID, Vcam)\n",
    "    json_directory = os.path.join(video_directory, 'json')\n",
    "\n",
    "    json_files = os.listdir(json_directory)\n",
    "    json_files.sort(key=lambda x: int(x.split('_')[2])) # sort by the frame id\n",
    "    \n",
    "\n",
    "    # the correct Openface csv file\n",
    "    feature_file = os.path.join('siblings_face', directory + '.csv')\n",
    "\n",
    "    # the label data for this video\n",
    "    label_value = label.iloc[i, 1:5] # 取1-4位的标签\n",
    "    video_id = label.iloc[i, 0] # video id\n",
    "\n",
    "    # Load face data of a video\n",
    "    try:\n",
    "        face_csv_data = pd.read_csv(feature_file)\n",
    "\n",
    "        # 丢弃置信度小于0.5的数据\n",
    "        face_csv_data = face_csv_data[face_csv_data['confidence'] >= confidence_threshold]\n",
    "\n",
    "        # 找到frame中有多于2个不同face_id的帧\n",
    "        counts = face_csv_data.groupby('frame')['face_id'].nunique()\n",
    "        frames_to_drop = counts[counts > 2].index.tolist()\n",
    "\n",
    "        # 对于每个需要处理的帧，删除置信度最低的行，直到只剩下2个face_id\n",
    "        for frame in frames_to_drop:\n",
    "            frame_df = face_csv_data[face_csv_data['frame'] == frame]\n",
    "            while frame_df['face_id'].nunique() > 2:\n",
    "                min_confidence = frame_df['confidence'].min()\n",
    "                rows_to_drop = frame_df[(frame_df['confidence'] == min_confidence)]['face_id'].tolist()\n",
    "                frame_df = frame_df[~frame_df['face_id'].isin(rows_to_drop)]\n",
    "            face_csv_data = face_csv_data[face_csv_data['frame'] != frame]\n",
    "            face_csv_data = pd.concat([face_csv_data, frame_df], ignore_index=True)\n",
    "\n",
    "        # 每一frame的 face feature\n",
    "        face_count += face_csv_data.shape[0]\n",
    "        features_face = face_csv_data.values \n",
    "    except:\n",
    "        print(f\"face error: {feature_file} not exist\")\n",
    "        continue\n",
    "    \n",
    "    pose_json_data = [] # save all frame data\n",
    "    # Load pose data of a video frame\n",
    "    for file in json_files: # for a single frame\n",
    "        file_path = os.path.join(json_directory, file)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # 漏检测 只有1人的情况\n",
    "            if len(data['people']) != 2:\n",
    "                row0 = data['people'][0]['pose_keypoints_2d']\n",
    "                # 插入face_id\n",
    "                row0.insert(0,0)\n",
    "                 # 插入frame\n",
    "                frame_id = int(file.split('_')[-2])//5 # 恢复frame id\n",
    "                row0.insert(0,frame_id)\n",
    "                pose_json_data.append(row0)\n",
    "\n",
    "                continue\n",
    "            \n",
    "            row0 = data['people'][0]['pose_keypoints_2d']\n",
    "            row1 = data['people'][1]['pose_keypoints_2d']\n",
    "\n",
    "\n",
    "            # face_id对应 x较小 标记为1，x较大 标记为0 \n",
    "            if(row0[0]<row1[0]):\n",
    "                row0,row1 = row1,row0 # swap\n",
    "\n",
    "            # 插入face_id\n",
    "            row0.insert(0,0)\n",
    "            row1.insert(0,1)\n",
    "\n",
    "            # 插入frame\n",
    "            frame_id = int(file.split('_')[-2])//5 + 1 # 恢复frame id \n",
    "            row0.insert(0,frame_id)\n",
    "            row1.insert(0,frame_id)\n",
    "            \n",
    "            pose_json_data.append(row0)\n",
    "            pose_json_data.append(row1)\n",
    "\n",
    "        except:\n",
    "            print(f\"pose error: {file} not exist\")\n",
    "            continue\n",
    "\n",
    "    pose_count += np.array(pose_json_data).shape[0]\n",
    "    features_pose = np.array(pose_json_data)\n",
    "\n",
    "    # check len\n",
    "    # print(f\"face:{len(face_csv_data)}, pose:{len(pose_json_data)}\")\n",
    "    \n",
    "    # 将两个数组合并为一个数组\n",
    "    for face in features_face:\n",
    "        for pose in features_pose:\n",
    "            if np.array_equal(face[:2], pose[:2]):\n",
    "                # 合并face和pose\n",
    "                features.append(np.concatenate((face[2:], pose[2:])))\n",
    "                # 重复labels\n",
    "                labels.append(np.argmax(label_value.astype(\"int\"),axis=0)) # one-hot to class id\n",
    "                frame2video_id.append(video_id)\n",
    "                break\n",
    "            else:\n",
    "                continue\n",
    "            break\n",
    "\n",
    "# 将特征和标签转换为numpy数组\n",
    "X = np.array(features)\n",
    "y = np.array(labels)\n",
    "\n",
    "# 总共遍历的face特征和pose特征数\n",
    "print(f\"Total face feat {face_count}, pose feat {pose_count}\")\n",
    "# 读入的数量\n",
    "print(f\"Read in X:{len(X)}, y:{len(y)}\")\n",
    "# drop\n",
    "print(f\"Drop {face_count - len(X)} face features, {pose_count - len(X)} pose features\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存为csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将X和Y合并为一个numpy数组\n",
    "data = np.concatenate((X, y), axis=1)\n",
    "\n",
    "# 将numpy数组转换为pandas DataFrame\n",
    "# df = pd.DataFrame(data, columns=[])\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# 将DataFrame保存为csv文件\n",
    "df.to_csv('res_data.csv', index=False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:789\n"
     ]
    }
   ],
   "source": [
    "X = np.array(features)\n",
    "print(f\"X:{len(X[0])}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# X_train = np.array(X_train)\n",
    "# X_test = np.array(X_test)\n",
    "# y_train = np.array(y_train)\n",
    "# y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.7895332390381895\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           强       0.70      0.91      0.79      1074\n",
      "           中       0.97      0.50      0.66       559\n",
      "           弱       0.85      0.80      0.82      1057\n",
      "    没有回应（忽视）       0.81      0.82      0.81       845\n",
      "\n",
      "    accuracy                           0.79      3535\n",
      "   macro avg       0.83      0.76      0.77      3535\n",
      "weighted avg       0.81      0.79      0.78      3535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 使用随机森林预测一个frame\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_predict = rfc.predict(X_test)\n",
    "\n",
    "print(f\"acc:{rfc.score(X_test, y_test)}\")\n",
    "print(classification_report(y_test, y_predict, target_names=[\"强\",\"中\",\"弱\",\"没有回应（忽视）\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "强：5270, 中：2895, 弱：5147, 没有回应（忽视）：4361\n"
     ]
    }
   ],
   "source": [
    "print(f\"强：{len(y[y==0])}, 中：{len(y[y==1])}, 弱：{len(y[y==2])}, 没有回应（忽视）：{len(y[y==3])}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.751060820367751\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           强       0.78      0.74      0.76      1074\n",
      "           中       0.67      0.68      0.67       559\n",
      "           弱       0.76      0.76      0.76      1057\n",
      "    没有回应（忽视）       0.76      0.80      0.78       845\n",
      "\n",
      "    accuracy                           0.75      3535\n",
      "   macro avg       0.74      0.74      0.74      3535\n",
      "weighted avg       0.75      0.75      0.75      3535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# 建立决策树分类器\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "print(f\"acc:{clf.score(X_test, y_test)}\")\n",
    "print(classification_report(y_test, y_predict, target_names=[\"强\",\"中\",\"弱\",\"没有回应（忽视）\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.792927864214993\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           强       0.78      0.82      0.80      1074\n",
      "           中       0.72      0.68      0.70       559\n",
      "           弱       0.80      0.78      0.79      1057\n",
      "    没有回应（忽视）       0.85      0.84      0.85       845\n",
      "\n",
      "    accuracy                           0.79      3535\n",
      "   macro avg       0.79      0.78      0.78      3535\n",
      "weighted avg       0.79      0.79      0.79      3535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 建立KNN分类器\n",
    "clf = KNeighborsClassifier(n_neighbors=3)\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "print(f\"acc:{clf.score(X_test, y_test)}\")\n",
    "print(classification_report(y_test, y_predict, target_names=[\"强\",\"中\",\"弱\",\"没有回应（忽视）\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "# clf = SVC(kernel='linear')\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "print(f\"acc:{clf.score(X_test, y_test)}\")\n",
    "print(classification_report(y_test, y_predict, target_names=[\"强\",\"中\",\"弱\",\"没有回应（忽视）\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.884016973125884\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'classification_report' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m\n\u001b[1;32m     23\u001b[0m accuracy \u001b[39m=\u001b[39m \u001b[39msum\u001b[39m(y_pred \u001b[39m==\u001b[39m y_test) \u001b[39m/\u001b[39m \u001b[39mlen\u001b[39m(y_test)\n\u001b[1;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mAccuracy:\u001b[39m\u001b[39m'\u001b[39m, accuracy)\n\u001b[0;32m---> 26\u001b[0m \u001b[39mprint\u001b[39m(classification_report(y_test, y_pred, target_names\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39m强\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m中\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m弱\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39m没有回应（忽视）\u001b[39m\u001b[39m\"\u001b[39m]))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'classification_report' is not defined"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# 定义模型参数\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 4,\n",
    "    'eval_metric': 'mlogloss'\n",
    "}\n",
    "\n",
    "# 将数据转换为DMatrix格式\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# 训练模型\n",
    "num_rounds = 100\n",
    "model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# 在测试集上预测\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "print('Accuracy:', accuracy)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"强\",\"中\",\"弱\",\"没有回应（忽视）\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           强       0.87      0.91      0.89      1074\n",
      "           中       0.91      0.76      0.83       559\n",
      "           弱       0.88      0.88      0.88      1057\n",
      "    没有回应（忽视）       0.89      0.93      0.91       845\n",
      "\n",
      "    accuracy                           0.88      3535\n",
      "   macro avg       0.89      0.87      0.88      3535\n",
      "weighted avg       0.88      0.88      0.88      3535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred, target_names=[\"强\",\"中\",\"弱\",\"没有回应（忽视）\"]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM  \n",
    "not working so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(100, 100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(100, 2) # 2 classes\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        out, _ = self.lstm(x) # 1 * 100\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "        \n",
    "class mil_regression(nn.Module):\n",
    "    def __init__(self, input_size=709, hidden_size=100, num_layers=1, output_size=2):\n",
    "        ''' use LSTM for MIL '''\n",
    "        super(mil_regression, self).__init__()\n",
    "        self.net = LSTM(input_size, hidden_size, num_layers)\n",
    "        self.class_num = output_size\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input shape: (batch_num, frame_num, feature_size)\n",
    "\n",
    "        self.b, self.seg_num, self.feature_num = inputs.shape #\n",
    "        outputs = torch.zeros((self.b, self.seg_num, self.class_num)).cuda() # batch * frame * 2（bool）\n",
    "\n",
    "        for i in range(self.seg_num):\n",
    "            outputs[:,i,:] = self.net(inputs[:,i,:]).squeeze()\n",
    "\n",
    "        # for idx, seg in enumerate(inputs):\n",
    "        #     seg = Variable(seg).cuda()\n",
    "        #     outputs[idx] = self.net(seg)\n",
    "\n",
    "        # 视频特征 = frame取平均\n",
    "        output = torch.mean(outputs, 1).cuda()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "model = mil_regression().cuda()\n",
    "\n",
    "epochs = 150\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for i in range(epochs):\n",
    "    for i in range(len(x_train)): # 每次处理一个视频(对batch)\n",
    "        x = x_train[i:i+10]\n",
    "        y = y_train[i:i+10]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_pred = model(x)\n",
    "\n",
    "        single_loss = loss_function(y_pred, y)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要分别读入face和pose的数据并对应起来\n",
    "- 对于siblings_face 需要读入json  \n",
    "- 对于siblings_pose 需要读入csv    \n",
    "- 对于Label 需要读入csv\n",
    "\n",
    "\n",
    "*特别注意对于face，需要去除置信度小于阈值的数据  \n",
    "*要将video的label扩展到每个frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. label1 回应情况"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# 读取标签文件\n",
    "\n",
    "# 1. 回应情况- ok\n",
    "label1_1 = pd.read_csv('labelA/回应情况-表格 1.csv')\n",
    "label1_2 = pd.read_csv('label_2/回应情况-表格 1.csv')\n",
    "# label1_3 = pd.read_csv('label_3/回应情况-表格 1.csv')\n",
    "\n",
    "label1 = pd.concat([label1_1, label1_2], axis=0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3127, 18)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0000'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label1.iloc[1,0][-4:]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. label2 互动情况积极与消极"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.互动情况的消极与积极\n",
    "label2_1 = pd.read_csv('labelA/互动情况的消极与积极-表格 1.csv')\n",
    "label2_2 = pd.read_csv('label_2/互动情况的消极与积极-表格 1.csv')\n",
    "\n",
    "label2 = pd.concat([label1_1, label1_2], axis=0)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. label3 任务专注"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3.任务专注\n",
    "label3_1 = pd.read_csv('labelA/任务专注-表格 1.csv')\n",
    "label3_2 = pd.read_csv('label_2/任务专注-表格 1.csv')\n",
    "\n",
    "label3 = pd.concat([label3_1, label3_2], axis=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. label4 任务内外的互动行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4.互动行为\n",
    "label4_1_1 = pd.read_csv('labelA/任务外的互动行为-表格 1.csv')\n",
    "label4_1_2 = pd.read_csv('labelA/任务中的互动行为-表格 1.csv')\n",
    "label4_2_1 = pd.read_csv('label_2/任务外的互动行为-表格 1.csv')\n",
    "label4_2_2 = pd.read_csv('label_2/任务中的互动行为-表格 1.csv')\n",
    "\n",
    "# 行连接\n",
    "label4_1 = pd.concat([label4_1_1, label4_1_2], axis=1)\n",
    "label4_2 = pd.concat([label4_2_1, label4_2_2], axis=1)\n",
    "\n",
    "label4 = pd.concat([label4_1, label4_2], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# missing features\n",
    "\n",
    "missing =[\"19YS_20230318_03/VCAM_0017_1\", \"19YS_20230318_03/VCAM_0016\", \"19YS_20230318_03/VCAM_0016_1\"]\n",
    "\n",
    "for i in missing:\n",
    "    label1 = label1[label1['切片ID'] != i]\n",
    "    label2 = label2[label2['切片ID'] != i]\n",
    "    label3 = label3[label3['切片ID'] != i]\n",
    "    label4 = label4[label4['切片ID'] != i]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并读取face和pose特征  \n",
    "按照视频读取frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_vel_and_acc(features):\n",
    "    \"\"\"\n",
    "    cal the mean and var of velocity and acceleration respectively of input features\n",
    "    \"\"\"\n",
    "    try:\n",
    "        first_order_derivatives = np.gradient(features, axis=0)\n",
    "        second_order_derivatives = np.gradient(first_order_derivatives, axis=0)\n",
    "    except:\n",
    "        print(\"Error in get_vel_and_acc\")\n",
    "        print(features.shape)\n",
    "        print(print(f\"len:{len(face_df0)} and {len(face_df1)}\"))\n",
    "        print(max_fm)\n",
    "        print(\"------------------\")\n",
    "\n",
    "    # velecity\n",
    "    vel_means = np.mean(first_order_derivatives, axis=0)\n",
    "    vel_var = np.var(first_order_derivatives, axis=0)\n",
    "\n",
    "    # acceleration\n",
    "    acc_means = np.mean(second_order_derivatives, axis=0)\n",
    "    acc_var = np.var(second_order_derivatives, axis=0)\n",
    "    \n",
    "    return vel_means, vel_var, acc_means, acc_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[face error] in siblings_face_1024/VCAM_0011_22.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0011_29.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0011_103.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0011_144.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0010_8.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0010_29.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0010_55.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0010_108.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0010_112.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_14.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_17.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_19.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_24.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_31.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_32.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_35.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_39.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_40.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_41.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_42.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_43.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_44.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_76.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_95.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0009_101.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0008_51.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0008_98.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0008_158.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0008_159.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0015_12.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0015_13.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0015_27.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0015_30.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0015_32.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0015_33.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0015_45.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0015_69.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0015_74.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0014.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0014_154.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0013_9.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0013_86.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0013_88.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0013_96.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0013_146.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0013_154.csv: not enough frames\n",
      "[pose error] in siblings_face_1024/VCAM_0013_165.csv: not enough frames\n",
      "[pose error]: VCAM_0013_166_000000000000_keypoints.json not exist\n",
      "[pose error] in siblings_face_1024/VCAM_0013_166.csv: not enough frames\n",
      "[pose error]: VCAM_0013_167_000000000000_keypoints.json not exist\n",
      "[pose error] in siblings_face_1024/VCAM_0013_167.csv: not enough frames\n",
      "[pose error]: VCAM_0013_168_000000000000_keypoints.json not exist\n",
      "[pose error] in siblings_face_1024/VCAM_0013_168.csv: not enough frames\n",
      "[pose error]: VCAM_0013_169_000000000000_keypoints.json not exist\n",
      "[pose error] in siblings_face_1024/VCAM_0013_169.csv: not enough frames\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_562253/2068437869.py:170: RuntimeWarning: invalid value encountered in arccos\n",
      "  angles = np.arccos(cos_angles)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[face error] in siblings_face_1024/VCAM_0018_7.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0018_59.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0018_101.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0018_126.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0017_10.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0017_59.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0016_47.csv: not enough frames\n",
      "[pose error] in siblings_face_1024/VCAM_0020.csv: not enough frames\n",
      "[pose error]: VCAM_0020_1_000000000000_keypoints.json not exist\n",
      "[pose error] in siblings_face_1024/VCAM_0020_1.csv: not enough frames\n",
      "[pose error]: VCAM_0021_000000000000_keypoints.json not exist\n",
      "[pose error] in siblings_face_1024/VCAM_0021.csv: not enough frames\n",
      "[pose error]: VCAM_0021_1_000000000000_keypoints.json not exist\n",
      "[pose error] in siblings_face_1024/VCAM_0021_1.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0021_9.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0021_17.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0021_18.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0021_61.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0023_10.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0023_13.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0023_25.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0023_30.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0023_48.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0024_136.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0024_137.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0025_21.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0027_11.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0027_28.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0027_48.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0027_73.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0027_137.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0027_139.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0027_161.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0027_165.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0028_41.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0028_42.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0028_43.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0028_55.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0028_57.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0028_58.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0028_59.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0028_122.csv: not enough frames\n",
      "[face error] in siblings_face_1024/VCAM_0029_121.csv: not enough frames\n",
      "Total face feat:936507, pose feat:192635\n",
      "Read in videos X:3032, face_feat:694744,pose_feat:191272\n",
      "Drop 241763 face features, 1363 pose features\n"
     ]
    }
   ],
   "source": [
    "# set openface confidence threshold\n",
    "confidence_threshold = 0.1\n",
    "\n",
    "# 导入特征并对齐标签\n",
    "low_video_features = []\n",
    "high_video_features = []\n",
    "video_labels = []\n",
    "labels = []\n",
    "frame2video_id = [] # 记录frame属于哪个video\n",
    "# 记录原本有多少条记录\n",
    "face_count = 0\n",
    "pose_count = 0\n",
    "# 记录读入的记录\n",
    "rface_count = 0\n",
    "rpose_count = 0\n",
    "\n",
    "# for each video\n",
    "for i in range(1,len(label)):\n",
    "    directory = label.iloc[i, 0]\n",
    "    frame_features = []\n",
    "    try:\n",
    "        # VCAM number\n",
    "        Date = directory.split('/')[0] # 15YS_20230317_01\n",
    "        Vcam = directory.split('/')[-1]  # VCAM_xxxx_xx\n",
    "        VcamID = Vcam.split('_')[1] # xxxx\n",
    "\n",
    "        # reconstruct the correct Openpose directory\n",
    "        video_directory = os.path.join('siblings_pose', Date,'VCAM_'+VcamID, Vcam)\n",
    "        json_directory = os.path.join(video_directory, 'json')\n",
    "    except:\n",
    "        print(f\"[load error]:{directory}\")\n",
    "\n",
    "    json_files = os.listdir(json_directory)\n",
    "    json_files.sort(key=lambda x: int(x.split('_')[2])) # sort by the frame id\n",
    "    \n",
    "\n",
    "    # the correct Openface csv file\n",
    "    # feature_file = os.path.join('siblings_face', directory + '.csv')\n",
    "    feature_file = os.path.join('siblings_face_1024', Vcam + '.csv')\n",
    "\n",
    "\n",
    "    # the label data for this video\n",
    "    label_value = label.iloc[i, 1:5] # 取1-4位的标签\n",
    "    video_id = i # video id\n",
    "\n",
    "    # Load face data of a video\n",
    "    try:\n",
    "        face_csv_data = pd.read_csv(feature_file)\n",
    "    except:\n",
    "        print(f\"[face error]: {feature_file} not exist\")\n",
    "        continue\n",
    "    \n",
    "    # log the num of face data rows\n",
    "    face_count += face_csv_data.shape[0]\n",
    "\n",
    "    # 丢弃置信度小于0.5的数据\n",
    "    face_csv_data = face_csv_data[face_csv_data['confidence'] >= confidence_threshold]\n",
    "\n",
    "    # TODO 丢弃 success = 0 的数据\n",
    "    face_csv_data = face_csv_data[face_csv_data['success'] ==1]\n",
    "\n",
    "    # 找到frame中有多于2个不同face_id的帧\n",
    "    counts = face_csv_data.groupby('frame')['face_id'].nunique()\n",
    "    frames_to_drop = counts[counts > 2].index.tolist()\n",
    "\n",
    "    \n",
    "    # 对于每个需要处理的帧，删除置信度最低的行，直到只剩下2个face_id\n",
    "    for frame in frames_to_drop:\n",
    "        frame_df = face_csv_data[face_csv_data['frame'] == frame]\n",
    "        while frame_df['face_id'].nunique() > 2:\n",
    "            min_confidence = frame_df['confidence'].min()\n",
    "            rows_to_drop = frame_df[(frame_df['confidence'] == min_confidence)]['face_id'].tolist()\n",
    "            frame_df = frame_df[~frame_df['face_id'].isin(rows_to_drop)]\n",
    "        face_csv_data = face_csv_data[face_csv_data['frame'] != frame]\n",
    "        face_csv_data = pd.concat([face_csv_data, frame_df], ignore_index=True)\n",
    "\n",
    "    # 丢弃只有1个face_id的帧 \n",
    "    # TODO 是否可以不丢弃？\n",
    "    face_csv_data = face_csv_data.groupby('frame').filter(lambda x: len(x) == 2)\n",
    "\n",
    "\n",
    "    # 按照frame分组，并根据x_30(nose x位置)的大小对face_id进行赋值 x较小标记为0\n",
    "    def assign_face_id(group):\n",
    "        if len(group) != 2:\n",
    "            return None\n",
    "        else:\n",
    "            if group['x_30'].iloc[0] < group['x_30'].iloc[1]:\n",
    "                group['face_id'] = [0, 1]\n",
    "            else:\n",
    "                group['face_id'] = [1, 0]\n",
    "            return group\n",
    "\n",
    "    face_csv_data = face_csv_data.groupby('frame').apply(assign_face_id).reset_index(drop=True)\n",
    "    # # 重新排序\n",
    "    # face_csv_data= face_csv_data.set_index(['face_id', 'frame'])\n",
    "\n",
    "    # def sort_by_frame(group):\n",
    "    #     return group.sort_values(by='frame')\n",
    "\n",
    "    # face_csv_data = face_csv_data.groupby('face_id').apply(sort_by_frame)\n",
    "\n",
    "\n",
    "    # 将df根据face_id列分成两个DataFrame对象\n",
    "    face_df0 = face_csv_data[face_csv_data['face_id'] == 0]\n",
    "    face_df1 = face_csv_data[face_csv_data['face_id'] == 1]\n",
    "\n",
    "    # 按照frame_id列进行排序\n",
    "    # face_df0 = face_df0.sort_values(by='frame')\n",
    "    # face_df1 = face_df1.sort_values(by='frame')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 将df的索引设置为frame_id,并且插值\n",
    "    try:\n",
    "        min_fm, max_fm = face_df0['frame'].min(), face_df0['frame'].max()\n",
    "        if(max_fm - min_fm >2):\n",
    "            # 记录contribute 数据数量\n",
    "            rface_count += 2*face_df0.shape[0]\n",
    "            # 插值\n",
    "            face_df0 = face_df0.set_index('frame').reindex(range(min_fm, max_fm+1)).interpolate()\n",
    "            face_df1 = face_df1.set_index('frame').reindex(range(min_fm, max_fm+1)).interpolate()\n",
    "        else:\n",
    "            print(f\"[face error] in {feature_file}: not enough frames\")\n",
    "            # print(f\"len:{len(face_df0)} and {len(face_df1)}\")\n",
    "            continue\n",
    "    except:\n",
    "        print(f\"[face error] in {feature_file}:unknown error in interpolation and reindex\")\n",
    "        continue\n",
    "    # # 对df进行插值\n",
    "    #  face_df0.reset_index()\n",
    "    \n",
    "    ###############  face low feature #######################\n",
    "    # low level feat to cal vel and acc\n",
    "    face_low_feat = ['pose_Tx', 'pose_Ty', 'pose_Tz','pose_Rx', 'pose_Ry', 'pose_Rz', 'gaze_angle_x', 'gaze_angle_y']\n",
    "    au_feat = ['AU04_r', 'AU05_r', 'AU06_r', 'AU07_r', 'AU09_r', 'AU15_r',  'AU20_r', 'AU23_r', 'AU26_r']\n",
    "    \n",
    "\n",
    "    # 每个人的low feat 的 vel and acc\n",
    "    v_mean_0, v_var_0, a_mean_0, a_var_0 = get_vel_and_acc(face_df0[face_low_feat])\n",
    "    v_mean_1, v_var_1, a_mean_1, a_var_1 = get_vel_and_acc(face_df1[face_low_feat])\n",
    "    au_mean_0 = np.array(face_df0[au_feat].mean())\n",
    "    au_mean_1 = np.array(face_df1[au_feat].mean())  \n",
    "    au_var_0 = np.array(face_df0[au_feat].var())\n",
    "    au_var_1 = np.array(face_df1[au_feat].var())\n",
    "\n",
    "\n",
    "    # 每一frame的 face feature\n",
    "    # features_face = face_csv_data.values \n",
    "    low_features_face_0 = np.concatenate((v_mean_0, v_var_0, a_mean_0, a_var_0,au_mean_0,au_var_0))\n",
    "    low_features_face_1 = np.concatenate((v_mean_1, v_var_1, a_mean_1, a_var_1,au_mean_1,au_var_1))\n",
    "\n",
    "    ###############  face high feature #######################\n",
    "\n",
    "    # @1 gaze_angle -> indicating gaze direction\n",
    "    gaze_angle_0 = np.array(face_df0[['gaze_angle_x','gaze_angle_y']])\n",
    "    gaze_angle_1 = np.array(face_df1[['gaze_angle_x','gaze_angle_y']])\n",
    "\n",
    "    gaze_mean_0 = gaze_angle_0.mean(axis=0)\n",
    "    gaze_mean_1 = gaze_angle_1.mean(axis=0) \n",
    "    gaze_var_0 = gaze_angle_0.var(axis=0)\n",
    "    gaze_var_1 = gaze_angle_0.var(axis=0)\n",
    "\n",
    "\n",
    "    # @2 计算两个人之间的角度夹角\n",
    "    # gaze_diff_x = face_df0['gaze_angle_x'] - face_df1['gaze_angle_x']\n",
    "    # gaze_diff_y = face_df0['gaze_angle_y'] - face_df1['gaze_angle_y']\n",
    "\n",
    "    cos_angles = np.sum(gaze_angle_0 * gaze_angle_1, axis=1) / (np.linalg.norm(gaze_angle_0, axis=1) * np.linalg.norm(gaze_angle_1, axis=1))\n",
    "    angles = np.arccos(cos_angles)\n",
    "    # 将弧度值转换为角度值\n",
    "    gaze_angles = np.degrees(angles)\n",
    "\n",
    "    gaze_angle_mean = gaze_angles.mean(axis=0)\n",
    "    gaze_angle_var = gaze_angles.var(axis=0) \n",
    "    v_mean_ga, v_var_ga, a_mean_ga, a_var_ga = get_vel_and_acc(gaze_angles)\n",
    "    # 单个数值合并成 array\n",
    "    gaze_feat = np.array([gaze_angle_mean, gaze_angle_var, v_mean_ga, v_var_ga, a_mean_ga, a_var_ga])\n",
    "\n",
    "\n",
    "    # @3 Motion synchronization -> abs\n",
    "    v_mean_ms, v_var_ms, a_mean_ms, a_var_ms = get_vel_and_acc((face_df0[face_low_feat] - face_df1[face_low_feat]).abs())\n",
    "\n",
    "\n",
    "    # 每一frame的 face feature\n",
    "    # features_face = face_csv_data.values \n",
    "    high_features_face = np.concatenate((gaze_mean_0, gaze_mean_1, gaze_var_0, gaze_var_1, \\\n",
    "                                         gaze_feat,\n",
    "                                         v_mean_ms, v_var_ms, a_mean_ms, a_var_ms\n",
    "                                        ))\n",
    "\n",
    "\n",
    "    ###############  pose feature #######################\n",
    "    \n",
    "    \n",
    "    pose_json_data = [] # save all frame data\n",
    "\n",
    "    # feature names in 'pose_keypoints_2d' in json file\n",
    "    col_name = [\n",
    "        'frame','face_id',\n",
    "       'x0', 'y0', 'c0', 'x1', 'y1', 'c1', 'x2', 'y2', 'c2', 'x3', 'y3', 'c3', 'x4', 'y4', 'c4',\n",
    "       'x5', 'y5', 'c5', 'x6', 'y6', 'c6', 'x7', 'y7', 'c7', 'x8', 'y8', 'c8',\n",
    "       'x9', 'y9', 'c9', 'x10', 'y10', 'c10', 'x11', 'y11', 'c11', 'x12', 'y12', 'c12',\n",
    "       'x13', 'y13', 'c13', 'x14', 'y14', 'c14', 'x15', 'y15', 'c15', 'x16', 'y16', 'c16', \n",
    "       'x17', 'y17', 'c17', 'x18', 'y18', 'c18', 'x19', 'y19', 'c19', 'x20', 'y20', 'c20', \n",
    "       'x21', 'y21', 'c21', 'x22', 'y22', 'c22', 'x23', 'y23', 'c23', 'x24', 'y24', 'c24'\n",
    "       ]\n",
    "\n",
    "    # 创建空的 dataframe\n",
    "    pose_df0 = pd.DataFrame(columns=col_name)\n",
    "    pose_df1 = pd.DataFrame(columns=col_name)\n",
    "    \n",
    "    \n",
    "    # Load pose data of a video frame\n",
    "    for file in json_files: # for a single frame\n",
    "        file_path = os.path.join(json_directory, file)\n",
    "        try:\n",
    "            with open(file_path, 'r') as f:\n",
    "                data = json.load(f)\n",
    "        except:\n",
    "            print(f\"[pose error]: {file} not exist\")\n",
    "            continue\n",
    "\n",
    "        # log the number\n",
    "        pose_count += len(data['people'])\n",
    "        \n",
    "        # 漏检测 只有1人的情况\n",
    "        if len(data['people']) != 2:\n",
    "            # drop this frame\n",
    "            continue\n",
    "        \n",
    "        row0 = data['people'][0]['pose_keypoints_2d'] # x0,y0,c0,x1,y1,c1,... x24,y24,c24\n",
    "        row1 = data['people'][1]['pose_keypoints_2d']\n",
    "\n",
    "        # face_id对应 x较小 标记为0，x较大 标记为1 \n",
    "        if(row0[0]>row1[0]):\n",
    "            row0,row1 = row1,row0 # swap\n",
    "\n",
    "        # 插入face_id\n",
    "        # TODO del face_id no need anymore\n",
    "        row0.insert(0,0)\n",
    "        row1.insert(0,1)\n",
    "\n",
    "        # 插入frame\n",
    "        frame_id = int(file.split('_')[-2]) # 文件 frame_id\n",
    "\n",
    "        # 去掉多余的最后一 frame 文件 否者会导致 frame 重复\n",
    "        if frame_id % 5 != 0:\n",
    "            continue\n",
    "\n",
    "        frame_id = frame_id // 5 + 1 # 恢复frame id 抽帧后的 id\n",
    "        row0.insert(0,frame_id)\n",
    "        row1.insert(0,frame_id)\n",
    "\n",
    "        pose_df0.loc[len(pose_df0)] = row0\n",
    "        pose_df1.loc[len(pose_df1)] = row1\n",
    "\n",
    "     # 将df的索引设置为frame_id,并且插值\n",
    "    try:\n",
    "        # set frame to int type\n",
    "        pose_df0['frame'] = pose_df0['frame'].astype(int)\n",
    "        pose_df1['frame'] = pose_df1['frame'].astype(int)\n",
    "        min_fm, max_fm = pose_df0['frame'].min(), pose_df0['frame'].max()\n",
    "\n",
    "        if(max_fm - min_fm >2):\n",
    "            # 记录contribute 数据数量\n",
    "            rpose_count += 2*pose_df0.shape[0]\n",
    "            # 插值\n",
    "            pose_df0 = pose_df0.set_index('frame').reindex(range(min_fm, max_fm+1)).interpolate()\n",
    "            pose_df1 = pose_df1.set_index('frame').reindex(range(min_fm, max_fm+1)).interpolate()\n",
    "        else:\n",
    "            print(f\"[pose error] in {feature_file}: not enough frames\")\n",
    "            # print(f\"len:{len(face_df0)} and {len(face_df1)}\")\n",
    "            continue\n",
    "    except:\n",
    "        print(f\"[pose error] in {feature_file}:unknown error in interpolation and reindex\")\n",
    "        continue\n",
    "\n",
    "    ###############  pose low feature #######################\n",
    "    \n",
    "    # TODO select low feat\n",
    "    # pose_low_feat = ['x','y']\n",
    "    pose_low_feat = col_name[2:]\n",
    "    pose_low_feat = [col for col in pose_low_feat if not col.startswith('c')] # filter all confidence\n",
    "\n",
    "    # 每个人的low feat 的 vel and acc\n",
    "    v_mean_0, v_var_0, a_mean_0, a_var_0 = get_vel_and_acc(pose_df0[pose_low_feat])\n",
    "    v_mean_1, v_var_1, a_mean_1, a_var_1 = get_vel_and_acc(pose_df1[pose_low_feat])\n",
    "    mean_0 = np.array(pose_df0[pose_low_feat].mean())\n",
    "    mean_1 = np.array(pose_df1[pose_low_feat].mean())\n",
    "    var_0 = np.array(pose_df0[pose_low_feat].var())\n",
    "    var_1 = np.array(pose_df1[pose_low_feat].var())\n",
    "    \n",
    "\n",
    "    ###############  pose high feature #######################\n",
    "    # TODO pose high feature\n",
    "\n",
    "    # # @1 left and right shoulder distance vel → (mean + var) = 2\n",
    "    # shoulder_lf_diff_0 = (pose_df0[['x2','y2']] - pose_df0[['x5','y5']]).abs()\n",
    "    # shoulder_lf_diff_1 = (pose_df1[['x2','y2']] - pose_df1[['x5','y5']]).abs()\n",
    "\n",
    "    # v_mean_lrs, v_var_lrs, a_mean_lrs, a_var_lrs = get_vel_and_acc(np.concatenate(shoulder_lf_diff_0,shoulder_lf_diff_1))\n",
    "\n",
    "    # # @2 质心靠近程度\n",
    "    # centroid_diff_x = np.abs(mean_0[0:-1:2].mean() - mean_1[0:-1:2].mean()) # 所有 x 平均(单数 index)\n",
    "    # centroid_diff_y = np.abs(mean_0[1:-1:2].mean() - mean_1[1:-1:2].mean()) # 所有 y 平均(双数 index)\n",
    "\n",
    "\n",
    "    # features_pose = np.array(pose_json_data)\n",
    "    low_features_pose_0 = np.concatenate((v_mean_0, v_var_0, a_mean_0, a_var_0,mean_0,var_0))\n",
    "    low_features_pose_1 = np.concatenate((v_mean_1, v_var_1, a_mean_1, a_var_1,mean_1,var_1))\n",
    "\n",
    "    # 将记录视频特征\n",
    "    low_video_features.append(np.concatenate((low_features_face_0, low_features_pose_0, low_features_face_1, low_features_pose_1))) # simply attach them all together\n",
    "    high_video_features.append(np.concatenate((high_features_face, low_features_face_0, low_features_pose_0, low_features_face_1, low_features_pose_1)))\n",
    "\n",
    "\n",
    "    # video 级别label\n",
    "    video_labels.append(np.argmax(label_value.astype(\"int\"),axis=0)) # one-hot to class id\n",
    "    \n",
    "    # # frame级别label\n",
    "    # video_labels.append(np.array(labels))\n",
    "\n",
    "X_low = low_video_features\n",
    "X_high = high_video_features\n",
    "\n",
    "y = video_labels\n",
    "\n",
    "\n",
    "# 总共遍历的face特征和pose特征数\n",
    "print(f\"Total face feat:{face_count}, pose feat:{pose_count}\")\n",
    "# 读入的数量\n",
    "print(f\"Read in videos X:{len(X_low)}, face_feat:{rface_count},pose_feat:{rpose_count}\")\n",
    "# # drop\n",
    "print(f\"Drop {face_count - rface_count} face features ({1 - rface_count/face_count:.2f}%), {pose_count - rpose_count} pose features({1 - rpose_count/pose_count:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total face feat:936507, pose feat:192635\n",
      "Read in videos X:3032, face_feat:694744,pose_feat:191272\n",
      "Drop 241763 face features (0.26%), 1363 pose features(0.01%)\n"
     ]
    }
   ],
   "source": [
    "# 总共遍历的face特征和pose特征数\n",
    "print(f\"Total face feat:{face_count}, pose feat:{pose_count}\")\n",
    "# 读入的数量\n",
    "print(f\"Read in videos X:{len(X_low)}, face_feat:{rface_count},pose_feat:{rpose_count}\")\n",
    "# # drop\n",
    "print(f\"Drop {face_count - rface_count} face features ({1 - rface_count/face_count:.2f}%), {pose_count - rpose_count} pose features({1 - rpose_count/pose_count:.2f}%)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 数据选择和预处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = high_video_features"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查 nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([940])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idx = np.where(np.isnan(X).any(axis=1)==True)[0]\n",
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(idx) != 0:\n",
    "    print(f\"drop {len(idx)} rows with nan value)}\")\n",
    "    for i in idx:\n",
    "        X = np.delete(X, i, axis=0)\n",
    "        y = np.delete(y, i, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_norm = np.apply_along_axis(lambda x: x / np.linalg.norm(x), axis=0, arr=X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64),)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(np.isnan(X_norm).any(axis=1)==True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read in videos y:3031\n"
     ]
    }
   ],
   "source": [
    "print(f\"Read in videos y:{len(y)}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X:746\n"
     ]
    }
   ],
   "source": [
    "print(f\"X:{len(X[0])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.min()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video num: X_train:2424, X_test:607, y_train:2424, y_test:607\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "# frame split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_norm, y, test_size=0.2, random_state=42)\n",
    "print(f\"Video num: X_train:{len(X_train)}, X_test:{len(X_test)}, y_train:{len(y_train)}, y_test:{len(y_test)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_of_cls(y): \n",
    "    print(f\"强：{len(y[y==0])}, 中：{len(y[y==1])}, 弱：{len(y[y==2])}, 没有回应（忽视）：{len(y[y==3])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y\n",
      "强：678, 中：416, 弱：1338, 没有回应（忽视）：599\n",
      "y_train\n",
      "强：536, 中：336, 弱：1061, 没有回应（忽视）：491\n",
      "y_test\n",
      "强：142, 中：80, 弱：277, 没有回应（忽视）：108\n"
     ]
    }
   ],
   "source": [
    "print(\"y\")\n",
    "num_of_cls(y)\n",
    "print(\"y_train\")\n",
    "num_of_cls(y_train)\n",
    "print(\"y_test\")\n",
    "num_of_cls(y_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation  \n",
    "- 5 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold, cross_val_score, cross_validate, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score, recall_score\n",
    "\n",
    "def crossVal(rfc, X, y):\n",
    "    kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "#     scoring = {'accuracy': make_scorer(accuracy_score),\n",
    "#             'f1': make_scorer(f1_score, average='weighted'),\n",
    "#             'recall': make_scorer(recall_score, average='weighted')}\n",
    "\n",
    "    scoring = ['accuracy', 'f1_weighted', 'recall_weighted', 'precision_weighted']\n",
    "\n",
    "    # 使用交叉验证器对模型进行评估\n",
    "    scores = cross_validate(rfc, X, y, cv=kfold, scoring=scoring)\n",
    "\n",
    "    # 输出交叉验证结果\n",
    "    print('Accuracy:', scores['test_accuracy'].mean())\n",
    "    print('F1 score:', scores['test_f1_weighted'].mean())\n",
    "    print('Recall:', scores['test_recall_weighted'].mean())\n",
    "    print('Precision:', scores['test_precision_weighted'].mean())\n",
    "\n",
    "def GridSearch(estimator, param_grid, X_train, y_train, X_test, y_test, cv=5):\n",
    "    grid_search = GridSearchCV(estimator=estimator, param_grid=param_grid, cv=cv)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # print(grid_search.best_params_, grid_search.best_score_)\n",
    "    print(\"Best hyperparameters:\", grid_search.best_params_)\n",
    "    print(\"Best cross-validation score:\", grid_search.best_score_)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    test_score = best_model.score(X_test, y_test)\n",
    "    print(\"Test score:\", test_score)\n",
    "    print(\"Report:\")\n",
    "    print(classification_report(y_test, y_predict, target_names=[\"强\",\"中\",\"弱\",\"没有回应（忽视）\"]))\n",
    "    return best_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.5222405271828665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           强       0.64      0.32      0.42       142\n",
      "           中       0.00      0.00      0.00        80\n",
      "           弱       0.50      0.91      0.65       277\n",
      "    没有回应（忽视）       0.55      0.19      0.29       108\n",
      "\n",
      "    accuracy                           0.52       607\n",
      "   macro avg       0.42      0.35      0.34       607\n",
      "weighted avg       0.48      0.52      0.45       607\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 使用随机森林预测一个frame\n",
    "rfc = RandomForestClassifier(n_estimators=100, max_depth=10)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_predict = rfc.predict(X_test)\n",
    "\n",
    "print(f\"acc:{rfc.score(X_test, y_test)}\")\n",
    "print(classification_report(y_test, y_predict, target_names=[\"强\",\"中\",\"弱\",\"没有回应（忽视）\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossVal(rfc,X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 50}\n",
      "Best cross-validation score: 0.4979398483428474\n",
      "Test score: 0.5107084019769358\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           强       0.64      0.32      0.42       142\n",
      "           中       0.00      0.00      0.00        80\n",
      "           弱       0.50      0.91      0.65       277\n",
      "    没有回应（忽视）       0.55      0.19      0.29       108\n",
      "\n",
      "    accuracy                           0.52       607\n",
      "   macro avg       0.42      0.35      0.34       607\n",
      "weighted avg       0.48      0.52      0.45       607\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# GridSearch\n",
    "param_grid = {\n",
    "    'n_estimators': [10, 50, 100, 200],\n",
    "    'max_depth': [None, 10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "RandomForest = GridSearch(estimator=rfc, param_grid=param_grid, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "发现问题：中类别通常会被忽略，导致了 warning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[y]         强：678, 中：416, 弱：1338, 没有回应（忽视）：599\n",
      "[y_train]   强：536, 中：336, 弱：1061, 没有回应（忽视）：491\n",
      "[y_test]    强：142, 中：80, 弱：277, 没有回应（忽视）：108\n",
      "[y_predict] 强：70, 中：0, 弱：499, 没有回应（忽视）：38\n"
     ]
    }
   ],
   "source": [
    "print(\"[y]\".ljust(12),end=\"\")\n",
    "num_of_cls(y)\n",
    "print(\"[y_train]\".ljust(12),end=\"\")\n",
    "num_of_cls(y_train)\n",
    "print(\"[y_test]\".ljust(12),end=\"\")\n",
    "num_of_cls(y_test)\n",
    "print(\"[y_predict]\".ljust(12),end=\"\")\n",
    "num_of_cls(y_predict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# 建立决策树分类器\n",
    "clf = DecisionTreeClassifier(random_state=0)\n",
    "# clf.fit(X_train, y_train)\n",
    "# y_predict = clf.predict(X_test)\n",
    "\n",
    "# print(f\"acc:{clf.score(X_test, y_test)}\")\n",
    "# print(classification_report(y_test, y_predict, target_names=[\"强\",\"中\",\"弱\",\"没有回应（忽视）\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.35907096774193553\n",
      "F1 score: 0.3549327998522269\n",
      "Recall: 0.35907096774193553\n",
      "Precision: 0.3589581231396566\n"
     ]
    }
   ],
   "source": [
    "crossVal(clf,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'criterion': 'gini', 'max_depth': 4, 'min_samples_leaf': 4, 'min_samples_split': 2}\n",
      "Best cross-validation score: 0.4628755218539661\n",
      "Test score: 0.45799011532125206\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           强       0.64      0.32      0.42       142\n",
      "           中       0.00      0.00      0.00        80\n",
      "           弱       0.50      0.91      0.65       277\n",
      "    没有回应（忽视）       0.55      0.19      0.29       108\n",
      "\n",
      "    accuracy                           0.52       607\n",
      "   macro avg       0.42      0.35      0.34       607\n",
      "weighted avg       0.48      0.52      0.45       607\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# GridSearch\n",
    "param_grid = {\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "    'max_depth': [4, 5, 6, 7, 8, 9, 10, 11, 12, 15, 20, 30, 40, 50, 70, 90, 120, 150],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4]\n",
    "}\n",
    "DT = GridSearch(estimator=clf, param_grid=param_grid, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# 建立KNN分类器\n",
    "clf = KNeighborsClassifier(n_neighbors=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3189032258064516\n",
      "F1 score: 0.3065399857800289\n",
      "Recall: 0.3189032258064516\n",
      "Precision: 0.3245333584546152\n"
     ]
    }
   ],
   "source": [
    "crossVal(clf,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch\n",
    "param_grid = {\n",
    "    'n_neighbors': list(range(1, 31)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2],\n",
    "    'leaf_size': list(range(1, 51))\n",
    "}\n",
    "GridSearch(estimator=clf, param_grid=param_grid, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "结果很差 看warning原因应该是有一些结果直接没有分类  \n",
    "看看是否需要调参数或者直接去掉 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc:0.416\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           强       0.40      0.62      0.48        39\n",
      "           中       0.00      0.00      0.00        19\n",
      "           弱       0.43      0.32      0.36        38\n",
      "    没有回应（忽视）       0.43      0.55      0.48        29\n",
      "\n",
      "    accuracy                           0.42       125\n",
      "   macro avg       0.32      0.37      0.33       125\n",
      "weighted avg       0.36      0.42      0.37       125\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "# clf = SVC(kernel='linear')\n",
    "clf = SVC()\n",
    "clf.fit(X_train, y_train)\n",
    "y_predict = clf.predict(X_test)\n",
    "\n",
    "print(f\"acc:{clf.score(X_test, y_test)}\")\n",
    "print(classification_report(y_test, y_predict, target_names=[\"强\",\"中\",\"弱\",\"没有回应（忽视）\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.3733161290322581\n",
      "F1 score: 0.3331413623531893\n",
      "Recall: 0.3733161290322581\n",
      "Precision: 0.31028633793320354\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ansonlo/anaconda3/envs/torch/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "crossVal(clf,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100, 1000],\n",
    "    'kernel': ['linear', 'poly', 'rbf', 'sigmoid'],\n",
    "    'gamma': ['scale', 'auto'] + list(np.logspace(-3, 3, 7))\n",
    "}\n",
    "\n",
    "GridSearch(estimator=clf, param_grid=param_grid, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.48\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           强       0.51      0.62      0.56        39\n",
      "           中       0.17      0.05      0.08        19\n",
      "           弱       0.47      0.53      0.49        38\n",
      "    没有回应（忽视）       0.52      0.52      0.52        29\n",
      "\n",
      "    accuracy                           0.48       125\n",
      "   macro avg       0.41      0.43      0.41       125\n",
      "weighted avg       0.45      0.48      0.46       125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "\n",
    "# 定义模型参数\n",
    "params = {\n",
    "    'objective': 'multi:softmax',\n",
    "    'num_class': 4,\n",
    "    'eval_metric': 'mlogloss'\n",
    "}\n",
    "\n",
    "# 将数据转换为DMatrix格式\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# 训练模型\n",
    "num_rounds = 100\n",
    "model = xgb.train(params, dtrain, num_rounds)\n",
    "\n",
    "# 在测试集上预测\n",
    "y_pred = model.predict(dtest)\n",
    "\n",
    "# 计算准确率\n",
    "accuracy = sum(y_pred == y_test) / len(y_test)\n",
    "print('Accuracy:', accuracy)\n",
    "print(classification_report(y_test, y_pred, target_names=[\"强\",\"中\",\"弱\",\"没有回应（忽视）\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.432658064516129\n",
      "F1 score: 0.40997846722685444\n",
      "Recall: 0.432658064516129\n",
      "Precision: 0.41474823524613236\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "# 定义xgboost分类器模型\n",
    "xgb = XGBClassifier(n_estimators=100, random_state=42)\n",
    "crossVal(xgb,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GridSearch\n",
    "param_grid = {\n",
    "    'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200, 500, 1000],\n",
    "    'max_depth': [3, 6, 10, 15],\n",
    "    'min_child_weight': [1, 3, 5, 7],\n",
    "    'gamma': [0, 0.1, 0.2, 0.3, 0.4],\n",
    "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "GridSearch(estimator=xgb, param_grid=param_grid, X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test, cv=5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM  \n",
    "not working so far"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self,input_size, hidden_size, num_layers):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size=input_size, hidden_size=hidden_size, num_layers=num_layers, batch_first=True)\n",
    "        self.fc1 = nn.Linear(100, 100)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(100, 4) # 4 classes\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(1)\n",
    "        out, _ = self.lstm(x) # 1 * 100\n",
    "        out = self.fc1(out[:, -1, :])\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n",
    "        \n",
    "class mil_regression(nn.Module):\n",
    "    def __init__(self, input_size=786, hidden_size=100, num_layers=1, output_size=1):\n",
    "        ''' use LSTM for MIL '''\n",
    "        super(mil_regression, self).__init__()\n",
    "        self.net = LSTM(input_size, hidden_size, num_layers)\n",
    "        self.class_num = output_size\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # input shape: (frame_num, feature_size)\n",
    "        \n",
    "\n",
    "        self.seg_num, self.feature_num = inputs.shape #\n",
    "\n",
    "        # outputs = torch.zeros((self.seg_num, self.class_num)).double.cuda() #  frame * 4（bool）\n",
    "\n",
    "        outputs = self.net(inputs)\n",
    "        # for i in range(self.seg_num):\n",
    "        #     outputs[i,:] = self.net(inputs[i]) # 786\n",
    "\n",
    "        # for idx, seg in enumerate(inputs):\n",
    "        #     seg = Variable(seg).cuda()\n",
    "        #     outputs[idx] = self.net(seg)\n",
    "\n",
    "        # 视频特征 = frame取平均\n",
    "        output = torch.mean(outputs, 1).cuda()\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "model = mil_regression().cuda()\n",
    "\n",
    "epochs = 150\n",
    "loss_function = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for i in range(len(X_train)): # 每次处理一个视频(对batch)\n",
    "\n",
    "        x = torch.tensor(X_train[i]).float().cuda()\n",
    "        y = torch.tensor(y_train[i]).float().cuda()\n",
    "        \n",
    "\n",
    "        if len(x.shape) ==1:\n",
    "            continue\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        y_pred = model(x)\n",
    "\n",
    "        single_loss = loss_function(y_pred, y)\n",
    "        single_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # if (i+1) % 10 == 0:\n",
    "        #     print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, epochs, i+1, len(X_train), single_loss.item()))\n",
    "\n",
    "    if i%25 == 1:\n",
    "        print(f'epoch: {i:3} loss: {single_loss.item():10.8f}')\n",
    "\n",
    "print(f'epoch: {i:3} loss: {single_loss.item():10.10f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for i in range(len(X_test)): # 每次处理一个视频(对batch)\n",
    "        x = torch.tensor(X_test[i]).float().cuda()\n",
    "        y = torch.tensor(y_test[i]).float().cuda()\n",
    "        print(y)\n",
    "\n",
    "        if len(x.shape) ==1:\n",
    "            continue\n",
    "\n",
    "        outputs = model(x)\n",
    "        print(outputs)\n",
    "        correct += (outputs == y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([[ 3. ,  3. ,  3. ],\n",
      "       [ 2. ,  3. , 48.5],\n",
      "       [ 0.5,  1.5, 47. ],\n",
      "       [ 0. ,  0. ,  0. ]]), array([[ 1. ,  1. ,  1. ],\n",
      "       [ 1. ,  1. ,  1. ],\n",
      "       [ 3. , 47.5, 92. ],\n",
      "       [ 3. , 47.5, 92. ]])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建一个二维矩阵\n",
    "x = np.array([[1, 2, 3], [4, 5, 6], [5, 8, 100],[5, 8, 100]])\n",
    "\n",
    "# 计算梯度\n",
    "dy = np.gradient(x)\n",
    "\n",
    "# 输出结果\n",
    "print(dy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([nan, nan])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_162420/1742590338.py:12: RuntimeWarning: invalid value encountered in divide\n",
      "  d2x.append(np.diff(dx2-dx1) / np.diff(dx3-dx2))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建一个二维数组\n",
    "x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 计算每一行相邻行之间的二阶导数\n",
    "d2x = []\n",
    "for i in range(1, x.shape[0]-1):\n",
    "    dx1 = np.gradient(x[i-1, :])\n",
    "    dx2 = np.gradient(x[i, :])\n",
    "    dx3 = np.gradient(x[i+1, :])\n",
    "    d2x.append(np.diff(dx2-dx1) / np.diff(dx3-dx2))\n",
    "\n",
    "# 输出结果\n",
    "print(d2x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([1., 1.]), array([1., 1.])]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 创建一个二维数组\n",
    "x = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "\n",
    "# 计算每一行数据对上一行数据的导数\n",
    "dx = []\n",
    "for i in range(1, x.shape[0]):\n",
    "    diff = np.diff(x[i, :]) / np.diff(x[i-1, :])\n",
    "    dx.append(diff)\n",
    "\n",
    "# 输出结果\n",
    "print(dx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   frame  feature\n",
      "0      1      1.0\n",
      "1      2      3.0\n",
      "2      3      5.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假设df是一个DataFrame对象，其中包含frame和feature两列\n",
    "df = pd.DataFrame({'frame': [1, 3], 'feature': [1, 5]})\n",
    "\n",
    "# 将df的索引设置为frame列，并增加需要插值的索引值\n",
    "df = df.set_index('frame').reindex(range(df['frame'].min(), df['frame'].max()+1))\n",
    "\n",
    "# 对df进行插值\n",
    "df_interpolated = df.interpolate().reset_index()\n",
    "\n",
    "# 输出插值结果\n",
    "print(df_interpolated)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1de83ba939c2ac04c4cd5f42769ee29bf75010bc15e3148e0552f0994dd76a71"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
